{"componentChunkName":"component---src-templates-post-js","path":"/zfs-slog-zil-revisited","result":{"data":{"markdownRemark":{"html":"<p>Taking a look back how my SLOG device has been performing on my ZFS pool after fixing some significant problems.</p>\n<!-- more -->\n<p>A few moons ago <a href=\"https://calvin.me/slow-vmware-nfs-zfs-add-zil/\">I recommended a SLOG/ZIL to improve NFS performance on ESXi</a>. At the time I was experiencing tremendously slow write speeds over NFS and adding a SLOG definitely fixed that but only covered up the real issue.</p>\n<h2 id=\"the-problem\" style=\"position:relative;\"><a href=\"#the-problem\" aria-label=\"the problem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Problem</h2>\n<p>I had really crap write speeds on my SSD pool. It was a mystery why as they were Samsung 840 512GB Pros.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Memory size: 16384 Megabytes\n\nwrite 12.8 GB via dd, please wait...\ntime dd if=/dev/zero of=/ssd/dd.tst bs=2048000 count=6250\n\n\nreal     2:08.5\nuser        0.0\nsys         1.5\n\n12.8 GB in 128.5s = 99.61 MB/s Write</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>My 6x4TB WD SE drives were performing smoothly however which made me believe it was really just a NFS issue on ESXi.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Memory size: 16384 Megabytes\n\nwrite 12.8 GB via dd, please wait...\ntime dd if=/dev/zero of=/hdd/dd.tst bs=2048000 count=6250\n\n6250+0 records in\n6250+0 records out\n12800000000 bytes transferred in 25.665755 secs (498719016 bytes/sec)\n\nreal       25.6\nuser        0.0\nsys         3.1\n\n12.8 GB in 25.6s = 500.00 MB/s Write</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2 id=\"the-temporary-solution-add-a-slog\" style=\"position:relative;\"><a href=\"#the-temporary-solution-add-a-slog\" aria-label=\"the temporary solution add a slog permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The (Temporary) Solution: Add a SLOG</h2>\n<p>As I wrote in my earlier post, I added a ZIL to my pool and it fixed everything. Write speeds were back up on ESXi and life was good. Deep down though, I knew something was still amiss.</p>\n<p>What I did not know until now was how the ZIL was fixing the issues. The way a ZIL works is that it when ESXi writes to the zpool datastore, it is actually writing to the ZIL. The ZIL would then flush what it has onto the zpool when it gets a good chance. This 'middle-man' in a way was what temporary solved the issue as it would not affect the performance I saw. Behind the scenes however the ZIL would have been waiting a low time to write to the zpool due to the crippling write speeds.</p>\n<h2 id=\"testing-and-how-i-came-to-the-solution\" style=\"position:relative;\"><a href=\"#testing-and-how-i-came-to-the-solution\" aria-label=\"testing and how i came to the solution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Testing and how I came to the solution</h2>\n<p>I tried a new installation of napp-it as recommended by its creator Gea. This showed no difference. <a href=\"http://hardforum.com/showpost.php?p=1042097373&#x26;postcount=7354\">He then advised</a> to disable sync and see if I was still seeing any difference. Nope no difference, actually it's even <em>SLOWER</em> than before.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Memory size: 16384 Megabytes\n\nwrite 4.096 GB via dd, please wait...\ntime dd if=/dev/zero of=/ssd-Bd0/dd.tst bs=4096000 count=1000\n\n1000+0 records in\n1000+0 records out\n4096000000 bytes transferred in 113.319963 secs (36145441 bytes/sec)\n\nreal     1:53.3\nuser        0.0\nsys         0.8\n\n4.096 GB in 113.3s = 36.15 MB/s Write</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Leaving napp-it and OmniOS behind I decided to run tests on Windows 2012 R2 using CrystalDiskMark and Samsung's very own Magician Tool and I was not surprised that I was getting good results.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 414px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c482b3737de0e3eea870a9e748d75b97/c6ea4/1-sgxocyq.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 92.01877934272301%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD8UlEQVQ4y5WQ2U8jBQDG59F3/QuMbxrXRIPxeMJo3Hg+CCEcWw6zRIX1wDUbWbIIsoiwh9utKywu3bIiWKi0pWBR2i1tWXrs9O50rrZzdTrDHKUnvahhjQ8++uV7+pJfvi8f8JnFes7pPesWPvam+11sv4vt2Y60671dBk/Lyv221b2ODbDLDH2wYu+1oooNX5s5Onxbe7fnzC8f9QPT9rAxiC4Hk+r9qA5ilnzYkhdZAZG7D8LaAKZ2BBbdMSOa3kDZzfiBCWGNCLsJUZYQqvdjwDpEHx/m8rLMk0xeEg/5tMimCrIoc1xBFtMkITBMtZCrFfPVQq5azNePirWjUr1cOigeAUacN9Gqm74RNTaBMpAgyHQqFYlClVpNEEUmlYonCRhFERSJwXAwFMkXi5VqlUuzpJQFDBinPGj+BH78AvUUk0OoBAVBMRRBQdAfDoVJim40GgccB4I+IknGUQz0emMxOCNLUqV+0kzKATBtC/MuVmIhDPZH/HASRigEIWGGpo8qZYqlglAQZ/AYBkWQKEagkiCI5RpgxDkl3vGh5dTF4Ol5/Iv+udfPad4ZmH2ve+qNBf/otuGvEOsZs7WeVb45pH330+XT59ffN3nuWf+0ybXGCbwpjNxKdt1jByyiajU8sRr8/vfYlA4d24rMhcEYfQib6CltYHItMqmDLxuob/eC23E0cVCqAEaMD3A2E7S6S5gDKbfZpzdY17cebOgdq6kMmcDiaYnbT1pMTp3ZtbENGh8mXWyaQWFYLNcBY5ybiL/cYn9sMPqkiuroufN853RT39xL7cpntc5bv2n0Xsp2ydvcOfNCx+SLitvPTWwqLDbzuk6fqT+ajWWtLkkHSiY04wlzNqTggHOOAGchJKRcrGZLUkxyhPn7cNYJZeyw6BYkoVws8cXyCbxMjI47+1TQkC6uHFkYmNBcuPzrl6Mrg/POce3SGpQK/eS4NDw7MLP51ZR+aM7zzR39jS2jOfPPYWqx5Wv06RnqNS1//pqz87qt96q1e/yP1sXAsG8/gsuBebx3xqq45lRctZ/5Yb9bv6v2eQJCuXoCJ4SQl3EF2Yc4j/jibhB3RVm/eMgRRJKmGTkro1w0TIM+1OOG96g0Wa1Uctnso7dx7juyud3xxOfwMz8Sfb0/N7WONw0uvO0O7aqUN69MX/ck7GO+t7pVryhmXm27cUprWomFoR2LNddoAAaMd+VndfJFc+aKR17bwRcscc0OtggnIIIkiSSdzpJ7gmYHU1sTmh18IZYMUXSKYRi+WAFMON9oNOrHx43/peO6VCoDi1GWyOYh8RCWsrCUO7H4r6U8LOYRKf+fUMwhUh4Vc6BQ+BsJwkHr4SJyiwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"1 - sgXocYq\"\n        title=\"1 - sgXocYq\"\n        src=\"/static/c482b3737de0e3eea870a9e748d75b97/c6ea4/1-sgxocyq.png\"\n        srcset=\"/static/c482b3737de0e3eea870a9e748d75b97/53f68/1-sgxocyq.png 213w,\n/static/c482b3737de0e3eea870a9e748d75b97/c6ea4/1-sgxocyq.png 414w\"\n        sizes=\"(max-width: 414px) 100vw, 414px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 850px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6ebd016acd17c81da6370a85a0955646/9cda9/2-umzqpia.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.36150234741784%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAADYUlEQVQozwFWA6n8ACA5ni9QrRlHqQdJrRtluF+VzFqa0Fmi1Fut2ly13Vm44Fm+4lnA4lnA4lvD5FrC41jA4VrC5GDB4WDA4AAkSJ02YakpYakGWqpDicP9+/n49vb18/P8+vr8+fn18/Dy8ur18+/7+Pn//v7//Pz69/f+/f3//v///v4ASXZHSnpGSoA/TYQ5gKV1+Pf78/T0/v//7u7u8vH20+nJrd2O0e24/////P39/f7+/////v//8/b59ff5AFJZY1NaZVJaYkZMVG91fPf3+fPy9P39/enp6fX19v3+/vz9/Pz9+/v8/PD0+PDz+Pb4+u3v9u3w9vf4+gBNVGVQV2lSWWtETF9sc4P7/fnt8ej4+ff////////z9u/v8uv+/v34+/70+f32+v7t9f3M6PrL6fvz9/sATFNjTlZnT1ZnQEhWbHN/7fHp2+PP6erm7+7w7e3t4ObV3ePS6uzn29/mytLdz9Pbtc/iVrHoY7Tl1OPuAE5VZExUY0tSY0FIVWdteJCXo3B6i3J+jGp2hWl1hWt2h2x3iGp2hmd0g2BvgF1tf2Zzgm90gWtuepmfqQBOVWNNVGVKUmI+RldoanZniKU8gLUuercmd7Ynd7UmdrUndrUod7Uod7YpeLYte7khc7Nvjqmoq7KZnqgASU9eR05cR01bPENRZ213mqOwd4aZf42ggYyfgIyfgY2gfoyfeomdfo2ghJOkgo2ff4udjZOfh4yWrrG5AE1TYkxTY0NKWTg/T2hve+/y9PDv8e7x89/u8t3q79jk6+jt8fX19fX09fL09t3r79jk6tLe5uHl6u7v8gBAR1Q7QlA7QlAxOEdhZ3Pa4OXS1dzN3uWFytN7u8xvpsWow9jX2t/P0tnP3eSNzdR5u8p0qMS+zdvk5uoAOT9MMzpHNj1LLTRCXWRv2NzgzM/Y1NzUrcyIs8ip1dTd0dbbz9TZzdHa1NzWudKUm8F6w83C0dXf4OToADA2RC81RDI3RicuPFthbOjs8Njc4tre49jf4eDl6uPo7uTp7uPn7OLn7OPo7eLo7OLo7ePo7uTp7+/y9gA2PEc0O0cyOUUwNkJCSFN/hIt/hIt+g4t7f4h6foZ5foZ5foZ7gIh8gYl7gIh9gYp7gIh8gol7gIh7gYgubSlwaCdpuAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"2 - UmzqPiA\"\n        title=\"2 - UmzqPiA\"\n        src=\"/static/6ebd016acd17c81da6370a85a0955646/d6170/2-umzqpia.png\"\n        srcset=\"/static/6ebd016acd17c81da6370a85a0955646/53f68/2-umzqpia.png 213w,\n/static/6ebd016acd17c81da6370a85a0955646/df70d/2-umzqpia.png 425w,\n/static/6ebd016acd17c81da6370a85a0955646/d6170/2-umzqpia.png 850w,\n/static/6ebd016acd17c81da6370a85a0955646/9cda9/2-umzqpia.png 1026w\"\n        sizes=\"(max-width: 850px) 100vw, 850px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>This made me believe there was something wrong with napp-it or OmniOS in general. I told Gea this info and he said it could be a <a href=\"http://hardforum.com/showpost.php?p=1042098980&#x26;postcount=7363\">problem with the disks, HBA/firmware or the mpt_sas driver</a> from OmniOS.</p>\n<h2 id=\"the-solution\" style=\"position:relative;\"><a href=\"#the-solution\" aria-label=\"the solution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Solution</h2>\n<p>Temporarily leaving napp-it behind I decided to try out FreeNAS and see if the issues still arose. A quick dd benchmark and test were very good, but not the 500MB/s I was seeing on Windows.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell-session\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-shell-session line-numbers\"><code class=\"language-shell-session\"><span class=\"token output\">[root@freenas] /mnt/ssd/dataset# dd if=/dev/zero of=/mnt/ssd/dataset/dd.testfile bs=4M count=10000\n10000+0 records in\n10000+0 records out\n41943040000 bytes transferred in 10.980550 secs (3819757645 bytes/sec)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Something was still wrong. I messed around in FreeNAS for a while (I would say napp-it has a lot more features but FreeNAS is cleaner, but i digress) and noticed some alerts in the top right hand corner. It told me I needed update my LSI 2308 controller from P19 to P20 (which FreeNAS supported). This was pretty much the same advice Gea had told me so I gave it a try and...BOOM</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Memory size: 16384 Megabytes\nwrite 12.8 GB via dd, please wait...\ntime dd if=/dev/zero of=/ssd/dd.tst bs=2048000 count=6250\n6250+0 records in\n6250+0 records out\n12800000000 bytes transferred in 23.141027 secs (553130172 bytes/sec)\n12.8 GB in 23.1s = 554.11 MB/s Write</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>The issue was fixed. 550MB/s write under dd benchmark.</p>\n<h2 id=\"retesting-and-results\" style=\"position:relative;\"><a href=\"#retesting-and-results\" aria-label=\"retesting and results permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Retesting and Results</h2>\n<p>Now that my write speeds were back to normal I then pondered on the need of a SLOG device anymore. So of course I ran some test.</p>\n<p>Here are the two tests I ran over NFS, ESXi does sync-writes so a SLOG is measurable here than using iSCSI.</p>\n<h3 id=\"ubuntu-14043-lts-server-installation\" style=\"position:relative;\"><a href=\"#ubuntu-14043-lts-server-installation\" aria-label=\"ubuntu 14043 lts server installation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ubuntu 14.04.3 LTS Server Installation</h3>\n<p>(But it uses the Internet to download during the Installation so not very accurate)</p>\n<ul>\n<li>With SLOG: 5:31 minutes</li>\n<li>Without SLOG: 9:18 minutes</li>\n</ul>\n<h3 id=\"windows-10-enterprise-ltsb-installation\" style=\"position:relative;\"><a href=\"#windows-10-enterprise-ltsb-installation\" aria-label=\"windows 10 enterprise ltsb installation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Windows 10 Enterprise LTSB Installation</h3>\n<p>(as soon as it begins writing to the disk)</p>\n<ul>\n<li>With SLOG: 4:20 minutes</li>\n<li>Without SLOG: 5:28 minutes</li>\n</ul>\n<p>The time difference was very large on Ubuntu however I could not pinpoint it down write speeds due to the installation requiring components to be downloaded over the Internet. Windows 10 however saw only a ~1 minute difference which is no big concern.</p>\n<h2 id=\"conclusion-no-slogzil-required\" style=\"position:relative;\"><a href=\"#conclusion-no-slogzil-required\" aria-label=\"conclusion no slogzil required permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion: No SLOG/ZIL required</h2>\n<p>I removed my SLOG from all my zpools and called it a day. The risk of running a SLOG outweighed the performance it brought along. As soon as the SLOG failed I knew I would have been in for it. Running a SLOG mirror on the other-hand is a viable option if money is not a issue. For a good SLOG, recommendations usually included the Intel S3700 ($300 new) or ZuesRAM drives ($1000+), neither of which were very affordable.</p>\n<p>Finding issues like these and fixing them are enjoyable especially when they are deeply layered and can change the performance of an entire system. Now if only I could solve all my Wi-Fi problems...</p>","timeToRead":5,"excerpt":"Taking a look back how my SLOG device has been performing on my ZFS pool after fixing some significant problems. A few moons ago I…","fileAbsolutePath":"/home/runner/work/calvin.me/calvin.me/master/posts/2016/2016-02-14-zfs-slog-zil-revisited/index.md","frontmatter":{"title":"ZFS SLOG/ZIL Drive (Revisited)","categories":["Storage"],"tags":["zfs","freenas","nas"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","backgroundColor":"#080808","images":{"fallback":{"src":"/static/5214a70344c3893dbc8c2a1546ae7e64/ccc8a/thumbnail.png","srcSet":"/static/5214a70344c3893dbc8c2a1546ae7e64/ccc8a/thumbnail.png 150w,\n/static/5214a70344c3893dbc8c2a1546ae7e64/67d4a/thumbnail.png 300w","sizes":"150px"},"sources":[{"srcSet":"/static/5214a70344c3893dbc8c2a1546ae7e64/3554d/thumbnail.webp 150w,\n/static/5214a70344c3893dbc8c2a1546ae7e64/c437f/thumbnail.webp 300w","type":"image/webp","sizes":"150px"}]},"width":150,"height":150}}}}}},"pageContext":{"filter":"/^.*\\/\\d{4}-\\d{2}-\\d{2}-zfs-slog-zil-revisited\\/.*$/"}},"staticQueryHashes":[]}